{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9447af9e",
   "metadata": {},
   "source": [
    "# Web Scrape Realtor Data for Properties in Bay Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc66cbc",
   "metadata": {},
   "source": [
    "As part of this workbook we will scrape the bay area for all the zips to fetch the data for the current resedential properties listes in \n",
    "1. 'Santa Clara County'\n",
    "2. 'San Mateo County'\n",
    "3. 'Alameda County'\n",
    "4. 'Contra Costa County'\n",
    "5. 'Marin County'\n",
    "6. 'Napa County'\n",
    "7. 'San Francisco County'\n",
    "8. 'Solano County'\n",
    "9. 'Sonoma County' \n",
    "\n",
    "which is essentially bay Area.\n",
    "\n",
    "### Goal\n",
    "\n",
    "The goal of this exercise is scrape the data and use it to understand the current listings and what is the price changes, listing pattern and more details around the zoning of the prices. \n",
    "\n",
    "Which zips are hot in the market now and how the general trend of the properties looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70e3e2",
   "metadata": {},
   "source": [
    "# Please Note - \n",
    "\n",
    "You need to make sure of following things before you execute this code and ensure the requirement are met.\n",
    "\n",
    "1. Ensure you have the zipcode and income data file in the same directory as of code. \n",
    "2. Make sure you change the flag value to False in the main function call main(False) if you are running for the first time. For second time, ensure you change it to false, so as to not consider the properties already fetched.\n",
    "3. Change the values of county_list variable in main fuction to run the code for the counties of your interest. \n",
    "\n",
    "\n",
    "\n",
    "Also the code does not stop in case of bad message from the website. THis was done in order to continue to scrape it even if one of the header was blocked. However, by the end the website might have made some changes which kind of blocking all the calls irrespective of the header from the same IP.\n",
    "\n",
    "Code will end abruplty if it faces issue with processing the links\n",
    "\n",
    "Should you have any questions please let us know.\n",
    "\n",
    "dsaraswat2@horizon.cseastbay.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9342a",
   "metadata": {},
   "source": [
    "### Import the libraries needed to connect and parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fbdb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below libraries are needed for the processing of the url requests, applying time functions for wait,\n",
    "# checking the status of the url request and beautiful soup to parse the response of the website response\n",
    "# json to process the formmatted ingested data from the website and pandas for mathematical operations\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import time  # Import the time module\n",
    "from urllib.request import urlopen\n",
    "import urllib.request \n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d718f3",
   "metadata": {},
   "source": [
    "# Function to fetch the soup URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faeae5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the function to fetch the status of the link passed and pass the extracted output/reponse as output\n",
    "## Function returns soup or none based on the response from the website.\n",
    "\n",
    "# The logic is to randomly select a header and use to ping to the server to get the resp. \n",
    "# if soup response is anything other than what BS can process it returns None.\n",
    "\n",
    "def home_for_sale_soup(link):\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE  # The above piece of three lines to ensure that website fakes us for a genuine request  and its a secure connection \n",
    "    url = link\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.50',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36 OPR/94.0.0.0'\n",
    "        #'Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)'\n",
    "\n",
    "    ]\n",
    "    try:\n",
    "        # Create the requests object and read the response from the wesbite in a object\n",
    "        usr_agent = random.choice(user_agents)\n",
    "        req = Request(url, headers={'User-Agent':usr_agent})\n",
    "        webpage = urlopen(req).read()\n",
    "        #print(webpage)\n",
    "        # Parse the object with Beautiful Soup with the html parser.\n",
    "        soup=BS(webpage,'html.parser')\n",
    "        return soup\n",
    "    except Exception  as e:\n",
    "        print(\"An error has occured:\", str(e))\n",
    "        print(\"This is the user agent used \", usr_agent)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b824f66",
   "metadata": {},
   "source": [
    "## Function to identify if the zipcode search has any property listed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45c411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the function to check for the zips where there are no properties listed. \n",
    "# It returns True if its None or else it returns False\n",
    "\n",
    "def check_if_no_results(soup):\n",
    "    no_results_find = soup.find(\"p\",{\"data-testid\":\"results-header-nomatch-results\"})\n",
    "    if no_results_find is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b27e47",
   "metadata": {},
   "source": [
    "## Function to Search Property urls on the search page results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358a62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function takes the search soup as input and search for the property page links and return a list of \n",
    "# the links of each property in the main page of the realtor.com. \n",
    "def links(soup):\n",
    "    try:\n",
    "        links = soup.find_all('div',{\"class\":\"card-image-wrapper\"}) #find all searches for the div tag with class\n",
    "        # as card-image-wrapper\n",
    "        list_of_links = []\n",
    "        for item in links:\n",
    "            list_of_links.append('https://www.realtor.com'+item.a.get('href').replace(\"?from=srp-list-card\",\"\"))\n",
    "        return list_of_links\n",
    "    except Exception as e:\n",
    "        print(\"Issue with fetching the links\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d0650",
   "metadata": {},
   "source": [
    "# Function to fetch the json data from the property soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51860e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_prop_url(soup):\n",
    "    try:\n",
    "        text_data = soup.find('script',{'type':'application/json'}).text\n",
    "        if text_data is not None:\n",
    "            return text_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Issue with fetching the links\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e58e25",
   "metadata": {},
   "source": [
    "## Function to get the number of pages for the property search result\n",
    "\n",
    "Some zip codes will have more than one page results so to identify all the pages we can look for the results feed and each page has 42 properties so we calculate the number of pages based on that and scan each zip result for that many pages.\n",
    "\n",
    "For example say one zip has more 45 properties then it will have two pages and hence the we will go to page 2 for the zipcode as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec27ba",
   "metadata": {},
   "source": [
    "## Function to build the list of urls for each page for a zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90bbc2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the links for all the pages for soup to be called. \n",
    "# We use the base url and pass the zipcode as input to build the links for a zipcode property search\n",
    "# Given that we may have more than 1 pages as search result output we will iterate over \n",
    "# the number of pages and build the url for each page of the of the zipcode search result.\n",
    "\n",
    "def build_zip_links(url, num_of_pages):\n",
    "    base_url = url\n",
    "    num_of_pages = num_of_pages\n",
    "    search_result_url_list = []\n",
    "    #Here iterate over the number of pages and if its 0 then build the links differently then for rest of the pages\n",
    "    for i in range(num_of_pages+1):\n",
    "        if i == 0 :\n",
    "            #link=base_url\n",
    "            #search_result_url_list.append(link)\n",
    "            continue\n",
    "        else : # Here append the /pg- page number to the link to get to the next page the increment is to \n",
    "            # ensure that given that pages start with pg2 and so forth and hence the pages are increases in such method\n",
    "            link = base_url+'/pg-'+str((i+1))\n",
    "            search_result_url_list.append(link)\n",
    "    return search_result_url_list\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed7b22",
   "metadata": {},
   "source": [
    "# Read zip_code data file and process it in pandas \n",
    "\n",
    "This function process the zipcode data file which has the zipcode and its details for all the areas in USA.\n",
    "\n",
    "We will fetch the data for only CA and clean some data issues. \n",
    "\n",
    "Then we will get the zipcode for the area of interest or counties as input to get the zipcode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad03aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process the zip_code data file and load the data for California and list of counties of interest to a dataFrame.\n",
    "\n",
    "def zip_file_processing(path, county_list):\n",
    "    \n",
    "    filename = path\n",
    "    zipcode_df = pd.read_csv(filename)\n",
    "    zipcode_df = zipcode_df[(zipcode_df[\"state\"] == \"CA\")]\n",
    "    zipcode_df[\"county\"].fillna('San Joaquin County', inplace=True)\n",
    "    zipcode_df[zipcode_df[\"county\"].isna()]\n",
    "    county_list = county_list\n",
    "    county_zipcode_df = zipcode_df[zipcode_df[\"county\"].isin(county_list)]\n",
    "    \n",
    "    return county_zipcode_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0564fdd2",
   "metadata": {},
   "source": [
    "# Function to write the data to a file\n",
    "\n",
    "Instead of opening a file and writing the data each time you need to write a file, use this function with the filename, data and file_type which will write the data to the file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b339e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to write the data in a file with the filetype as well. \n",
    "## The fucntionality of the filetype has been removed later, however the variable remained to support the underline code.\n",
    "\n",
    "def write_data_to_file(filename, data,file_type):\n",
    "    filename = filename\n",
    "    data = data\n",
    "    file_type = file_type\n",
    "    with open(filename,'a') as file:\n",
    "        file.write(data+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "386dfbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_pages_for_zip(soup):\n",
    "    soup = soup\n",
    "    try:\n",
    "        results = soup.find('div',{\"data-testid\":\"total-results\"}).text #find all searches for the div tag with class\n",
    "        #print(results)\n",
    "        # as card-image-wrapper\n",
    "        if results is not None:\n",
    "            num_of_pages = int(results)/42\n",
    "            #print(num_of_pages)\n",
    "            return num_of_pages\n",
    "        else: \n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"Issue with fetching the counts\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4805f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate(filename,outfile):\n",
    "    try:\n",
    "        unique_line = set()\n",
    "        with open(filename,'r') as file:\n",
    "            for line in file:\n",
    "                unique_line.add(line.strip())\n",
    "            \n",
    "        with open(outfile,'w') as file1:\n",
    "            for line in unique_line:\n",
    "                file1.write(line+'\\n')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577b2eb5",
   "metadata": {},
   "source": [
    "# Process Flow to Fetch the links for all the properties\n",
    "\n",
    "We will process the zip_code_database.csv (\"zip data\") to fetch the zipcodes for the counties of interest. \n",
    "\n",
    "For those zip code, we will create the URL for searching properties listed on realtor.com.\n",
    "\n",
    "We will iterate over all the zipcode search result pages and fetch the links for the properties listed on those pages. \n",
    "\n",
    "Those fetched links will be stored in a property_links.csv file which will finally be used later to fetch the details for each individual property for data about -\n",
    "\n",
    "Property details, price, type , tax history, price history, school details, neighbourhood details and much more. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668c5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch links for all the properties for the zip code in the counties list.\n",
    "\n",
    "def web_scrape_search_pages(base_url, zip_file_path, csv_file_path, property_link_file_path,\n",
    "                           county_list):\n",
    "    \n",
    "    # Initialize the variables here for the function to run the code. \n",
    "    base_url = base_url\n",
    "    zip_file_path = zip_file_path\n",
    "    csv_file_path = csv_file_path\n",
    "    property_link_file_path= property_link_file_path\n",
    "    county_list = county_list\n",
    "    \n",
    "    url_list = []\n",
    "    # Load the zipcode data for a each county iteratively and get the zipcodes.\n",
    "    county_zipcode_df = zip_file_processing(zip_file_path, county_list)\n",
    "    \n",
    "    try:\n",
    "        # Now start building the urls for each of the zipcodes iteratively\n",
    "        for county in county_list:\n",
    "            zips = county_zipcode_df.loc[county_zipcode_df[\"county\"] == county,[\"zip\",\"primary_city\",\"county\"]]\n",
    "            print(\"Working on county : \", county)\n",
    "            zip_codes = list(zips[\"zip\"])\n",
    "            # Build the links for each zipcode for base url search\n",
    "            for i in zip_codes:\n",
    "                time.sleep(random.randint(3, 6))\n",
    "                url = base_url + str(i)\n",
    "                print(\"Working for zipcode : \", i)\n",
    "                # Check the soup for that zip code using the home_for_sale_soup function by passing the link\n",
    "                soup = home_for_sale_soup(url)\n",
    "                #print(soup)\n",
    "\n",
    "                # Check the return object and notice if we have received the proper response.\n",
    "                if soup is not None:\n",
    "                    #print(\"Hello\")\n",
    "\n",
    "                    # First check if we have any results for that zipcode or note if not then continue to next zip\n",
    "                    if check_if_no_results(soup):\n",
    "                        print(\"There is no results for the zipcode : \", i)\n",
    "                        continue\n",
    "\n",
    "                    # parse the data for the main page first and check the number of pages on the main page -\n",
    "\n",
    "                    num_pages = num_of_pages_for_zip(soup)\n",
    "                    #print(num_pages)\n",
    "                    if num_pages is None:\n",
    "                        print(\"Could not fetch the number of pages for zip:\",i)\n",
    "                        raise Exception(\"Sorry, could not fetch the numbers\")  \n",
    "\n",
    "                    # Get the links for the properties for the main page already downloaded. \n",
    "                    # My approach is to store the links in a list here as well as load the links to a csv file as well\n",
    "                    # This to ensure that we have skim through these links and properties have been scanned.\n",
    "\n",
    "                    #write_data_to_file(csv_file_path,url,'csv')\n",
    "\n",
    "                    # Get the links for the properties of this page -\n",
    "                    prop_links = links(soup)\n",
    "\n",
    "                    #Check if the links are fetched properly or not\n",
    "                    if prop_links is not None:\n",
    "                        for item in prop_links:\n",
    "                            write_data_to_file(property_link_file_path,item,'csv')\n",
    "\n",
    "                    else:\n",
    "                        print(\"Breaking the loop due to error at links of property fetch\")\n",
    "                        raise Exception(\"Issue with property links fetch for \", i)\n",
    "                        break\n",
    "\n",
    "                    # Now I have the number of pages and zip code as well so I will call the build the url for the zipcode\n",
    "                    if num_pages > 1.0 : # if the number of pages are more than the one page then build the links for all the pages\n",
    "\n",
    "                        zip_code_next_pgs_links = build_zip_links(url,int(num_pages))\n",
    "                        # Lets iterate over the constructed zipcode list of links for all pages\n",
    "                        for item in zip_code_next_pgs_links: # For all the pages write the link to the zipcode links\n",
    "                            write_data_to_file(csv_file_path,item,'csv')\n",
    "                            # Now call the soup to get the property url from each page using item.\n",
    "                            page_soup = home_for_sale_soup(item)\n",
    "                            if page_soup is not None:\n",
    "                                #get the property url's\n",
    "                                page_prop_link = links(page_soup)\n",
    "                                if page_prop_link is not None:\n",
    "                                    for i in page_prop_link:\n",
    "                                        write_data_to_file(property_link_file_path,i,'csv')\n",
    "                                else:\n",
    "                                    print(\"Breaking the inner loop due to error at the links of property fetch\")\n",
    "                                    break\n",
    "                            else:\n",
    "                                print(\"Issues with getting the soup let's break the loop and get out\")\n",
    "                                break\n",
    "\n",
    "                    else:\n",
    "                        print(f\"Issue with processing the zipcode for {i} \")\n",
    "                        break\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"Issue with data processing , exiting abruptly for zip code : \",i)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a7fea",
   "metadata": {},
   "source": [
    "## Function to remove the data from property links data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c3eb9c",
   "metadata": {},
   "source": [
    "## Main function\n",
    "\n",
    "As part of this function we will call the process flow. \n",
    "\n",
    "The code to fetch the data for the individual properties has been not put into the function to avoid the continues running and have more adhoce changes made to the flow for the need basis\n",
    "\n",
    "Here you should be focusing on updating the values for the following variables in the main function -\n",
    "\n",
    "1. county_list --> it is a list of all the counties for whom you need to fetch the details for the properties.\n",
    "in our case it is for the CA bay area counties.\n",
    "2. result_passed --> You should pass its values as True or False while calling the main function. \n",
    "\n",
    "You should pass it as False for the first time call to make sure it searches for the properties listed in each zip code else it will fetch for those properties\n",
    "\n",
    "After one run when you have all the links we can call this function many a times by passing the values as True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d02b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables and call the function to test the process flow.\n",
    "def main(result_passed):\n",
    "    base_url = \"https://www.realtor.com/realestateandhomes-search/\"\n",
    "\n",
    "    #Get the zipcodes from by reading from the zipcode data file. \n",
    "    zip_file_path = \"zip_code_database.csv\"\n",
    "    # Define the files to write the zipcode links \n",
    "    csv_file_path = 'zip_code_links.csv'\n",
    "    property_link_file_path = 'property_links.csv'\n",
    "    \n",
    "    # Change the county list as mentioned in the comments underline to fetch the data for counties in California\n",
    "    #county_list = ['Santa Clara County','San Mateo County',\n",
    "    #               'Alameda County', # Done with this as well\n",
    "    #               'Contra Costa County','Marin County','Napa County', # are alreadys scanned in first pass\n",
    "    #               'San Francisco County',  # Done with SFO will probably scan it later as well\n",
    "    #               'Solano County','Sonoma County']\n",
    "    county_list=['Solano County']\n",
    "    \n",
    "    # Delete the data from duplicate data from the property_links file.\n",
    "\n",
    "    filename = 'property_links.csv'\n",
    "    outfile = 'property_links_clean.csv'\n",
    "    property_data_dir = 'property_data/'\n",
    "\n",
    "    \n",
    "    #result = web_scrape_search_pages(base_url, zip_file_path, csv_file_path, property_link_file_path,county_list)\n",
    "    if result_passed is True:\n",
    "        result = True\n",
    "    else:\n",
    "        result = web_scrape_search_pages(base_url, zip_file_path, csv_file_path, property_link_file_path,county_list)\n",
    "    \n",
    "    # I am keeping the results as True always to ensure that we have some links to process always.\n",
    "\n",
    "    if result:\n",
    "        # Remove the duplicate properties from the properties links\n",
    "        if remove_duplicate(filename,outfile):\n",
    "            \n",
    "            # Check if we have the folder already created or not. If not then create the folder here.\n",
    "            os.makedirs(property_data_dir,exist_ok=True)\n",
    "            \n",
    "            \n",
    "            # Do the processing for fetching the links, it could have been a function call instead we write the full\n",
    "            # full code. This is in order to have control to manually kill the code to avoid IP bns.\n",
    "            \n",
    "            with open(outfile,'r') as file3:\n",
    "                for line in file3:\n",
    "                    # For each link call the soup function get the soup.\n",
    "                    # Again we will sleep for random time before we make a new request. \n",
    "                    url = line.strip()\n",
    "                    prop_file_name = property_data_dir+url.rsplit('/',1)[-1]+'.json'\n",
    "                    # Now let's call the random function to get the sleep before a new request. \n",
    "                    #time.sleep(random.randint(2,6))\n",
    "                    #time.sleep(20)\n",
    "                    try:\n",
    "                        prop_soup = home_for_sale_soup(url)\n",
    "                        #print(prop_soup)\n",
    "                        if prop_soup is not None:\n",
    "                            # Call the soup parser to fetch the data fields. \n",
    "                            data = data_from_prop_url(prop_soup) # Get the fields data from the soup\n",
    "\n",
    "                            json_data = json.loads(data)  # Convert the data to a json data \n",
    "\n",
    "                            #outcome = get_data_fields_for_property(json_data) # Get the data fields from that JSON\n",
    "\n",
    "                            print(\"writing for property: \",prop_file_name)\n",
    "\n",
    "                            with open(prop_file_name,mode='w') as file:\n",
    "                                json.dump(json_data,file)\n",
    "                                file.write('\\n')\n",
    "                                #print(\"Write the content for the file correctly,\", url)\n",
    "                            #time.sleep(random.randint(4,10))\n",
    "\n",
    "                        else:\n",
    "                            print(\"Issue with fetching the data for the property\",url)\n",
    "                            time.sleep(random.randint(15,16))\n",
    "                            continue\n",
    "                    except Exception as e:\n",
    "                        print(\"Issue with the soup fetching \", e, url)\n",
    "                        time.sleep(random.randint(15,16))\n",
    "                        continue\n",
    "            #### This is where the fetching should stop\n",
    "        else:\n",
    "            print(\"Issue with removing the duplicated from file : \", filename)\n",
    "    else:\n",
    "        print(\"We should never come here !!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323bdb8",
   "metadata": {},
   "source": [
    "## Call the Main function\n",
    "\n",
    "Call the main function to fetch the properties data based on the status of your current execution.\n",
    "\n",
    "I have set the value as True for now, you should change it to False if you are running it for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7853379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error has occured: HTTP Error 403: Forbidden\n",
      "This is the user agent used  Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.50\n",
      "Issue with fetching the data for the property https://www.realtor.com/realestateandhomes-detail/482-Panorama-Dr_Benicia_CA_94510_M24989-57457\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Change the value to False should you be executing my code for the first time. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This will fetch the data for the zip codes.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m### This could have been automated, however in the interest of time and the scope of project I have limited\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m### its capacity of execution to allow more control over let the code do stuff manually\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m main(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[12], line 75\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(result_passed)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;66;03m#print(\"Write the content for the file correctly,\", url)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;66;03m#time.sleep(random.randint(4,10))\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIssue with fetching the data for the property\u001b[39m\u001b[38;5;124m\"\u001b[39m,url)\n\u001b[0;32m---> 75\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m16\u001b[39m))\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Change the value to False should you be executing my code for the first time. \n",
    "# This will fetch the data for the zip codes.\n",
    "\n",
    "#Note- Please make sure you have the zip-code file in the same folder as the code. \n",
    "#Note2- Please kill the code manually if it starts showing you 403 or any such errors. \n",
    "\n",
    "# The reason to keep code killing manual is to ensure that we do not get into IP block status\n",
    "### This could have been automated, however in the interest of time and the scope of project I have limited\n",
    "### its capacity of execution to allow more control over let the code do stuff manually\n",
    "\n",
    "main(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397047f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = home_for_sale_soup('https://www.realtor.com/realestateandhomes-search/94510')\n",
    "results = num_of_pages_for_zip(soup)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc253c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
